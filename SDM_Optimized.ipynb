{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Optimized Species Distribution Modeling (SDM)\n",
        "\n",
        "This notebook contains a streamlined and modularized version of the SDM workflow.\n",
        "It includes steps for initialization, data preparation, model training, prediction, and export."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Setup and Imports\n",
        "import ee\n",
        "import geemap\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.cloud import bigquery\n",
        "import geopandas as gpd\n",
        "import shapely.geometry\n",
        "import json\n",
        "\n",
        "# Initialize Earth Engine\n",
        "try:\n",
        "    ee.Initialize(project='cryptic-yen-457008-p4')\n",
        "except Exception as e:\n",
        "    ee.Authenticate()\n",
        "    ee.Initialize(project='cryptic-yen-457008-p4')\n",
        "\n",
        "#  Initialize BigQuery\n",
        "client = bigquery.Client(project='rsc-cropmap-lzp')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. Run the Query\n",
        "sql = \"\"\"\n",
        "SELECT\n",
        "    pred1 AS species,\n",
        "    CAST(agri_year AS INT64) AS year,\n",
        "    ST_AsGeoJSON(ST_CENTROID(geometry)) AS geometry\n",
        "FROM\n",
        "    `rsc-cropmap-lzp.published.Fused_Categories_Orchards`\n",
        "WHERE\n",
        "    -- Use specific optimization filter if partition pruning is needed, \n",
        "    -- otherwise keep standard filters to reduce scanned data if possible.\n",
        "    pred1 IS NOT NULL\n",
        "QUALIFY\n",
        "    -- This looks at ALL years for a specific geometry at once\n",
        "    LOGICAL_AND(pred1 = 'avocado') OVER (PARTITION BY geometry)\n",
        "\"\"\"\n",
        "print(\"Running BigQuery...\")\n",
        "df = client.query(sql).to_dataframe(create_bqstorage_client=False)\n",
        "print(f\"Rows retrieved: {len(df)}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. Convert to GeoDataFrame\n",
        "df['geometry'] = df['geometry'].apply(lambda x: shapely.geometry.shape(json.loads(x)))\n",
        "gdf = gpd.GeoDataFrame(df, geometry='geometry', crs=\"EPSG:4326\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4. Convert to Earth Engine FeatureCollection (Chunked)\n",
        "print(\"Converting to Earth Engine object...\")\n",
        "\n",
        "def gdf_to_ee_chunked(gdf, chunk_size=2000):\n",
        "    \"\"\"Uploads GDF in chunks to avoid payload limits.\"\"\"\n",
        "    fcs = []\n",
        "    print(f\"Total rows: {len(gdf)}. Uploading in chunks of {chunk_size}...\")\n",
        "    for i in range(0, len(gdf), chunk_size):\n",
        "        chunk = gdf.iloc[i:i+chunk_size]\n",
        "        fc = geemap.gdf_to_ee(chunk)\n",
        "        fcs.append(fc)\n",
        "\n",
        "    print(\"Merging chunks on server...\")\n",
        "    # Merge sequentially to avoid deep nesting if list is long\n",
        "    if not fcs:\n",
        "        return ee.FeatureCollection([])\n",
        "\n",
        "    merged = fcs[0]\n",
        "    for fc in fcs[1:]:\n",
        "        merged = merged.merge(fc)\n",
        "\n",
        "    return merged\n",
        "\n",
        "# Use chunked upload\n",
        "data_raw = gdf_to_ee_chunked(gdf)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. Configuration\n",
        "# Centralize all constants and paths here for easy management\n",
        "\n",
        "CONFIG = {\n",
        "    'BANDS': ['OrderST', 'aspect', 'elevation', 'slope', 'bio01', 'bio12', 'srad', 'vdf'],\n",
        "    'ASSETS': {\n",
        "        'SOIL': \"projects/cryptic-yen-457008-p4/assets/SDM/IsraelSoilTaxonomy\",\n",
        "        'BIO1': \"projects/cryptic-yen-457008-p4/assets/SDM/wc2_1_30s_bio_1\",\n",
        "        'BIO12': \"projects/cryptic-yen-457008-p4/assets/SDM/wc2_1_30s_bio_12\",\n",
        "        'SRAD': \"projects/cryptic-yen-457008-p4/assets/SDM/wc2_1_30s_srad\",\n",
        "        'VAPR': \"projects/cryptic-yen-457008-p4/assets/SDM/wc2_1_30s_vapr\",\n",
        "        'MODEL_OUTPUT': 'projects/cryptic-yen-457008-p4/SDM/assets/avocado_final_model'\n",
        "    },\n",
        "    'CMIP6_COLLECTION': \"NASA/GDDP-CMIP6\",\n",
        "    'VISUALIZATION': {\n",
        "        'SUITABILITY': {\"min\": 0, \"max\": 1, \"palette\": [\"ffffff\", \"cecece\", \"fcd163\", \"66a000\", \"204200\"]},\n",
        "        'DIFF': {\"min\": -0.3, \"max\": 0.3, \"palette\": [\"d7191c\", \"ffffff\", \"2c7bb6\"]}\n",
        "    },\n",
        "    'EXPORT': {\n",
        "        'FOLDER': 'GEE_Exports',\n",
        "        'SCALE': 1000\n",
        "    },\n",
        "    'GRAIN_SIZE': 1000,\n",
        "    'TEST_YEAR': 2018\n",
        "}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. Helper Functions\n",
        "\n",
        "def get_predictors():\n",
        "    \"\"\"Loads and preprocesses predictor variables.\"\"\"\n",
        "    # Topography\n",
        "    terrain = ee.Algorithms.Terrain(ee.Image(\"USGS/SRTMGL1_003\")).unmask()\n",
        "\n",
        "    # Soil\n",
        "    soil_fc = ee.FeatureCollection(CONFIG['ASSETS']['SOIL'])\n",
        "    u_types = soil_fc.aggregate_array('OrderST').distinct().sort()\n",
        "    soil_img = soil_fc.map(lambda f: f.set('Code', u_types.indexOf(f.get('OrderST')))) \\\n",
        "        .reduceToImage(['Code'], ee.Reducer.first()).rename('OrderST').unmask(-1)\n",
        "\n",
        "    # Climate (Current)\n",
        "    # Load new assets\n",
        "    # FIX: Removed .unmask() to avoid 0 values where data is missing.\n",
        "    # Missing data will be dropped during sampling, which is better than training on 0s.\n",
        "    bio1 = ee.Image(CONFIG['ASSETS']['BIO1']).rename('bio01')\n",
        "    bio12 = ee.Image(CONFIG['ASSETS']['BIO12']).rename('bio12')\n",
        "    srad = ee.Image(CONFIG['ASSETS']['SRAD']).rename('srad')\n",
        "    vapr = ee.Image(CONFIG['ASSETS']['VAPR']).rename('vapr')\n",
        "\n",
        "    # Calculate VDF (Vapor Pressure Deficit)\n",
        "    # VPD = es - ea (vapr)\n",
        "    # es = 0.6108 * exp(17.27 * T / (T + 237.3))\n",
        "    # bio1 is Mean Temp (check units, assuming Celsius based on standard WorldClim)\n",
        "\n",
        "    es = bio1.expression(\n",
        "        '0.6108 * exp((17.27 * T) / (T + 237.3))',\n",
        "        {'T': bio1}\n",
        "    )\n",
        "\n",
        "    # vapr is in kPa, es is in kPa (0.6108 is kPa)\n",
        "    vdf = es.subtract(vapr).rename('vdf')\n",
        "\n",
        "    # Combine all\n",
        "    return bio1.addBands([bio12, srad, vdf, soil_img, terrain.select(['elevation', 'slope', 'aspect'])])\n",
        "\n",
        "def remove_duplicates(data, grain_size):\n",
        "    \"\"\"Removes duplicate presence points within the same pixel.\"\"\"\n",
        "    random_raster = ee.Image.random().reproject(\"EPSG:4326\", None, grain_size)\n",
        "    rand_point_vals = random_raster.sampleRegions(\n",
        "        collection=ee.FeatureCollection(data), geometries=True\n",
        "    )\n",
        "    return rand_point_vals.distinct(\"random\")\n",
        "\n",
        "def split_data(data, predictors, aoi, test_year, grain_size):\n",
        "    \"\"\"Splits data into Train, Validation, and Test sets with pseudo-absences.\"\"\"\n",
        "\n",
        "    # 1. De-duplicate Presence\n",
        "    print(\"Removing duplicates...\")\n",
        "    presence = remove_duplicates(data, grain_size)\n",
        "    print(f\"Presence points after de-duplication: {presence.size().getInfo()}\")\n",
        "\n",
        "    # 2. Split Presence by Year and Random\n",
        "    # Test Set (Hold out year)\n",
        "    pres_test = presence.filter(ee.Filter.eq('year', test_year)).map(lambda f: f.set('PresAbs', 1))\n",
        "\n",
        "    # Remaining (Train + Val)\n",
        "    pres_remain = presence.filter(ee.Filter.neq('year', test_year))\n",
        "    pres_remain = pres_remain.randomColumn()\n",
        "\n",
        "    # Train (70%) / Val (30%)\n",
        "    pres_train = pres_remain.filter(ee.Filter.lt('random', 0.7)).map(lambda f: f.set('PresAbs', 1))\n",
        "    pres_val = pres_remain.filter(ee.Filter.gte('random', 0.7)).map(lambda f: f.set('PresAbs', 1))\n",
        "\n",
        "    # 3. Generate Pseudo-Absences\n",
        "    print(\"Generating pseudo-absences...\")\n",
        "\n",
        "    # Presence mask (user logic)\n",
        "    presence_mask = presence.reduceToImage(properties=['random'], reducer=ee.Reducer.first()) \\\n",
        "        .reproject('EPSG:4326', None, grain_size).mask().neq(1).selfMask()\n",
        "\n",
        "    # Valid predictor area (mask of first band)\n",
        "    cl_mask = predictors.select(0).mask()\n",
        "\n",
        "    # Area for Pseudo-Absences\n",
        "    area_for_pa = presence_mask.updateMask(cl_mask).clip(aoi)\n",
        "\n",
        "    # Generate absences (Total count approx equal to total presence)\n",
        "    total_pres_count = presence.size()\n",
        "    # FIX: Cast to Int to avoid float error\n",
        "    num_pixels = total_pres_count.multiply(1.2).toInt()\n",
        "\n",
        "    absences = predictors.sample(\n",
        "        region=area_for_pa.geometry(),\n",
        "        scale=grain_size,\n",
        "        numPixels=num_pixels,\n",
        "        geometries=True\n",
        "    ).randomColumn().map(lambda f: f.set('PresAbs', 0))\n",
        "\n",
        "    # Split Absences to match Presence ratios\n",
        "    # We want roughly 1:1 ratio in each set\n",
        "    count_test = pres_test.size()\n",
        "    count_train = pres_train.size()\n",
        "\n",
        "    # Sort by random to easily pick chunks\n",
        "    absences_list = absences.toList(absences.size())\n",
        "\n",
        "    abs_test = ee.FeatureCollection(absences_list.slice(0, count_test))\n",
        "    abs_train = ee.FeatureCollection(absences_list.slice(count_test, count_test.add(count_train)))\n",
        "    abs_val = ee.FeatureCollection(absences_list.slice(count_test.add(count_train)))\n",
        "\n",
        "    # 4. Merge and Sample\n",
        "    def sample_data(pres, abs_):\n",
        "        merged = pres.merge(abs_)\n",
        "        return predictors.select(CONFIG['BANDS']).sampleRegions(\n",
        "            collection=merged,\n",
        "            properties=[\"PresAbs\"],\n",
        "            scale=grain_size,\n",
        "            tileScale=16\n",
        "        )\n",
        "\n",
        "    train_data = sample_data(pres_train, abs_train)\n",
        "    val_data = sample_data(pres_val, abs_val)\n",
        "    test_data = sample_data(pres_test, abs_test)\n",
        "\n",
        "    return train_data, val_data, test_data\n",
        "\n",
        "def train_model(training_data, mode='MULTIPROBABILITY', n_trees=500, min_leaf=10):\n",
        "    \"\"\"rains the Random Forest classifier\"\"\"\n",
        "\n",
        "    print(f\"Training Random Forest: Trees={n_trees}, MinLeaf={min_leaf}...\")\n",
        "    classifier = ee.Classifier.smileRandomForest(\n",
        "        numberOfTrees=n_trees, \n",
        "        minLeafPopulation=min_leaf\n",
        "    ).train(training_data, \"PresAbs\", CONFIG['BANDS'])\n",
        "    return classifier.setOutputMode(mode)\n",
        "\n",
        "def get_future_climate(scenario, model='ACCESS-CM2', year=2050):\n",
        "    \"\"\"Fetches and processes future climate data.\"\"\"\n",
        "    \n",
        "    start_year = year - 9\n",
        "    end_year = year + 10\n",
        "    \n",
        "    nex = ee.ImageCollection(CONFIG['CMIP6_COLLECTION']) \\\n",
        "        .filter(ee.Filter.date(f'{start_year}-01-01', f'{end_year}-12-31')) \\\n",
        "        .filter(ee.Filter.eq('scenario', scenario)) \\\n",
        "        .filter(ee.Filter.eq('model', model))\n",
        "        \n",
        "    # Check if collection is empty (client-side check to avoid errors)\n",
        "    count = nex.size().getInfo()\n",
        "    if count == 0:\n",
        "        print(f\"WARNING: No images found for {scenario} {year} (Model: {model}). Returning empty image.\")\n",
        "        return ee.Image().rename('empty')\n",
        "\n",
        "    def convert(img):\n",
        "        # Pr is in kg m-2 s-1 (mm/s). Multiply by 86400 to get mm/day.\n",
        "        pr = img.select('pr').multiply(86400).rename('precip_mm')\n",
        "        \n",
        "        # Tas is in Kelvin. Subtract 273.15 to get Celsius.\n",
        "        tas = img.select('tas').subtract(273.15).rename('tmean_c')\n",
        "        \n",
        "        # Rsds is in W m-2. Convert to kJ m-2 day-1.\n",
        "        # 1 W = 1 J/s. 1 day = 86400 s.\n",
        "        # W/m2 * 86400 = J/m2/day. Divide by 1000 for kJ.\n",
        "        # Factor = 86.4\n",
        "        srad = img.select('rsds').multiply(86.4).rename('srad')\n",
        "        \n",
        "        # Calculate VPD using Tas and Hurs (%)\n",
        "        # es = 0.6108 * exp(17.27 * T / (T + 237.3))\n",
        "        # ea = es * (hurs / 100)\n",
        "        # vpd = es - ea\n",
        "        \n",
        "        t = tas\n",
        "        hurs = img.select('hurs')\n",
        "        \n",
        "        es = t.expression(\n",
        "            '0.6108 * exp((17.27 * T) / (T + 237.3))',\n",
        "            {'T': t}\n",
        "        )\n",
        "        \n",
        "        ea = es.multiply(hurs.divide(100))\n",
        "        vdf = es.subtract(ea).rename('vdf')\n",
        "        \n",
        "        return img.addBands([pr, tas, srad, vdf])\n",
        "\n",
        "    # FIX: Use EPSG:3857 (meters) so that '1000' is interpreted as 1000 meters.\n",
        "    # EPSG:4326 uses degrees, so '1000' was 1000 degrees (huge pixels).\n",
        "    nex_agg = nex.map(convert).map(lambda i: i.resample('bilinear').reproject('EPSG:3857', None, 1000))\n",
        "    \n",
        "    # Aggregate over time\n",
        "    bio01 = nex_agg.select('tmean_c').mean().rename('bio01')\n",
        "    bio12 = nex_agg.select('precip_mm').mean().multiply(365).rename('bio12')\n",
        "    srad = nex_agg.select('srad').mean().rename('srad')\n",
        "    vdf = nex_agg.select('vdf').mean().rename('vdf')\n",
        "    \n",
        "    return bio01.addBands([bio12, srad, vdf])\n",
        "\n",
        "def export_image_to_drive(image, description, filename, region):\n",
        "    \"\"\"Creates and starts an export task.\"\"\"\n",
        "    task = ee.batch.Export.image.toDrive(\n",
        "        image=image,\n",
        "        description=description,\n",
        "        folder=CONFIG['EXPORT']['FOLDER'],\n",
        "        fileNamePrefix=filename,\n",
        "        scale=CONFIG['EXPORT']['SCALE'],\n",
        "        region=region,\n",
        "        maxPixels=1e13\n",
        "    )\n",
        "    task.start()\n",
        "    print(f\"Started export task: {description}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4. Main Execution Flow\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- A. Prepare Data ---\n",
        "print(\"Preparing data...\")\n",
        "predictors = get_predictors()\n",
        "soil_fc = ee.FeatureCollection(CONFIG['ASSETS']['SOIL'])\n",
        "aoi = soil_fc.geometry().bounds()\n",
        "\n",
        "if 'data_raw' in locals():\n",
        "    print(\"Splitting data and preparing datasets...\")\n",
        "    # data_raw is the raw EE FeatureCollection from BigQuery\n",
        "    train_data, val_data, test_data = split_data(\n",
        "        data_raw, predictors, aoi, CONFIG['TEST_YEAR'], CONFIG['GRAIN_SIZE']\n",
        "    )\n",
        "\n",
        "    # DEBUG: Check sizes and Class Balance\n",
        "    print(\"Checking dataset sizes...\")\n",
        "    try:\n",
        "        print(f\"Training set size: {train_data.size().getInfo()}\")\n",
        "        print(f\"Validation set size: {val_data.size().getInfo()}\")\n",
        "        print(f\"Test set size: {test_data.size().getInfo()}\")\n",
        "\n",
        "        # DEBUG: Check Class Balance\n",
        "        print(\"Training Class Distribution:\", train_data.aggregate_histogram('PresAbs').getInfo())\n",
        "\n",
        "        # DEBUG: Check first element of training data\n",
        "        first_feat = train_data.first().getInfo()\n",
        "        print(\"First training sample properties:\", first_feat['properties'].keys())\n",
        "    except Exception as e:\n",
        "        print(f\"WARNING: Could not get dataset info (Timeout?): {e}\")\n",
        "\n",
        "    print(\"Training model...\")\n",
        "    # Train with default (MULTIPROBABILITY) for later use in Maps\n",
        "    rf_model = train_model(train_data)\n",
        "\n",
        "    # --- Validation ---\n",
        "    print(\"Validating model...\")\n",
        "    # FIX: Use CLASSIFICATION mode for error matrix (0/1 labels instead of probabilities)\n",
        "    rf_model_class = rf_model.setOutputMode('CLASSIFICATION')\n",
        "\n",
        "    validated = val_data.classify(rf_model_class)\n",
        "\n",
        "    # --- DEEP DEBUGGING ---\n",
        "    print(\"Inspecting validation results...\")\n",
        "    try:\n",
        "        # Check if 'classification' band exists and what values it has\n",
        "        first_val = validated.first().getInfo()\n",
        "        print(\"First validated feature properties:\", first_val['properties'].keys())\n",
        "        if 'classification' in first_val['properties']:\n",
        "            print(\"First validated classification value:\", first_val['properties']['classification'])\n",
        "            print(\"First validated PresAbs value:\", first_val['properties']['PresAbs'])\n",
        "        else:\n",
        "            print(\"WARNING: 'classification' property missing from validated data!\")\n",
        "\n",
        "        # Print raw confusion matrix\n",
        "        error_matrix = validated.errorMatrix('PresAbs', 'classification')\n",
        "        matrix_vals = error_matrix.array().getInfo()\n",
        "        print(\"Confusion Matrix:\", matrix_vals)\n",
        "\n",
        "        print(\"Validation Accuracy:\", error_matrix.accuracy().getInfo())\n",
        "        print(\"Validation Kappa:\", error_matrix.kappa().getInfo())\n",
        "    except Exception as e:\n",
        "        print(f\"Validation Debugging Failed: {e}\")\n",
        "        print(\"Exporting results instead.\")\n",
        "        # Export Validation Metrics\n",
        "        val_metrics = ee.FeatureCollection([\n",
        "            ee.Feature(None, {\n",
        "                'accuracy': error_matrix.accuracy(),\n",
        "                'kappa': error_matrix.kappa(),\n",
        "                'matrix': error_matrix.array()\n",
        "            })\n",
        "        ])\n",
        "        ee.batch.Export.table.toDrive(\n",
        "            collection=val_metrics,\n",
        "            description='Validation_Metrics',\n",
        "            folder=CONFIG['EXPORT']['FOLDER'],\n",
        "            fileFormat='CSV'\n",
        "        ).start()\n",
        "\n",
        "    # --- Testing ---\n",
        "    print(f\"Testing on year {CONFIG['TEST_YEAR']}...\")\n",
        "    tested = test_data.classify(rf_model_class) # Use CLASSIFICATION mode here too\n",
        "    test_matrix = tested.errorMatrix('PresAbs', 'classification')\n",
        "\n",
        "    try:\n",
        "        print(\"Test Accuracy:\", test_matrix.accuracy().getInfo())\n",
        "    except Exception as e:\n",
        "        print(\"Test accuracy timed out. Exporting results instead.\")\n",
        "        # Export Test Metrics\n",
        "        test_metrics = ee.FeatureCollection([\n",
        "            ee.Feature(None, {\n",
        "                'accuracy': test_matrix.accuracy(),\n",
        "                'matrix': test_matrix.array()\n",
        "            })\n",
        "        ])\n",
        "        ee.batch.Export.table.toDrive(\n",
        "            collection=test_metrics,\n",
        "            description='Test_Metrics',\n",
        "            folder=CONFIG['EXPORT']['FOLDER'],\n",
        "            fileFormat='CSV'\n",
        "        ).start()\n",
        "\n",
        "else:\n",
        "    print(\"WARNING: 'data_raw' variable not found. Please ensure BigQuery step ran successfully.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Data Analysis ---\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Define a safe conversion function\n",
        "def ee_to_pandas_safe(fc):\n",
        "    \"\"\"Robust conversion from EE FeatureCollection to Pandas DataFrame.\"\"\"\n",
        "    try:\n",
        "        # Try standard geemap function\n",
        "        return geemap.ee_to_pandas(fc)\n",
        "    except AttributeError:\n",
        "        try:\n",
        "            # Try alias (common in some versions)\n",
        "            return geemap.ee_to_df(fc)\n",
        "        except AttributeError:\n",
        "            # Manual fallback using getInfo (works everywhere)\n",
        "            features = fc.getInfo()['features']\n",
        "            data = []\n",
        "            for f in features:\n",
        "                props = f['properties']\n",
        "                data.append(props)\n",
        "            return pd.DataFrame(data)\n",
        "\n",
        "if 'train_data' in locals():\n",
        "    print(\"Generating Correlation Matrix...\")\n",
        "\n",
        "    # Limit size\n",
        "    n_samples = min(5000, train_data.size().getInfo())\n",
        "    print(f\"Sampling {n_samples} points for analysis...\")\n",
        "\n",
        "    # 2. Use the safe function here\n",
        "    df_train = ee_to_pandas_safe(train_data.limit(n_samples))\n",
        "\n",
        "    # Select predictor columns\n",
        "    cols = CONFIG['BANDS']\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(df_train[cols].corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "    plt.title(\"Predictor Correlation Matrix\")\n",
        "    plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- DEBUG: Check Data Values ---\n",
        "if 'df_train' in locals():\n",
        "    print(\"Checking 'bio01' and 'elevation' values...\")\n",
        "    print(df_train[['bio01', 'elevation']].describe())\n",
        "\n",
        "    # Scatter plot to see the relationship\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.scatter(df_train['elevation'], df_train['bio01'], alpha=0.5)\n",
        "    plt.xlabel('Elevation (m)')\n",
        "    plt.ylabel('Bio01 (Temp)')\n",
        "    plt.title('Elevation vs Bio01')\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    # Check for zeros (unmask issue)\n",
        "    zeros = (df_train['bio01'] == 0).sum()\n",
        "    print(f\"Number of 0 values in bio01: {zeros} out of {len(df_train)}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Model Interpretation ---\n",
        "if 'rf_model' in locals():\n",
        "    print(\"Calculating Variable Importance...\")\n",
        "    importance = rf_model.explain().get('importance').getInfo()\n",
        "\n",
        "    # Plot\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.bar(importance.keys(), importance.values())\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.title(\"Variable Importance\")\n",
        "    plt.ylabel(\"Importance\")\n",
        "    plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- C. Future Predictions ---\n",
        "\n",
        "def predict_suitability(model, climate_stack, static_predictors):\n",
        "    full_stack = climate_stack.addBands(static_predictors.select(['OrderST', 'elevation', 'slope', 'aspect']))\n",
        "    return full_stack.select(CONFIG['BANDS']).classify(model).arrayGet([1])\n",
        "\n",
        "if 'rf_model' in locals():\n",
        "    print(\"Predicting future scenarios...\")\n",
        "\n",
        "    scenarios = ['ssp245', 'ssp585']\n",
        "    years = [2050, 2100]\n",
        "\n",
        "    future_maps = {}\n",
        "\n",
        "    for scenario in scenarios:\n",
        "        for year in years:\n",
        "            print(f\"Processing {scenario} - {year}...\")\n",
        "            future_climate = get_future_climate(scenario, year=year)\n",
        "            suitability_map = predict_suitability(rf_model, future_climate, predictors)\n",
        "            future_maps[f\"{scenario}_{year}\"] = suitability_map\n",
        "\n",
        "    # Calculate Difference (Example: SSP585 2050 vs SSP245 2050)\n",
        "    if 'ssp585_2050' in future_maps and 'ssp245_2050' in future_maps:\n",
        "        diff_map = future_maps['ssp585_2050'].subtract(future_maps['ssp245_2050'])\n",
        "        print(\"Difference map created.\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5. Visualization\n",
        "Map = geemap.Map(layout={'height':'600px', 'width':'100%'})\n",
        "Map.centerObject(aoi, 7)\n",
        "\n",
        "# Add Current Suitability\n",
        "if 'rf_model' in locals() and 'predictors' in locals():\n",
        "    print(\"Generating Current Suitability Map...\")\n",
        "    # rf_model is in MULTIPROBABILITY mode by default from train_model()\n",
        "    # We want the probability of class 1 (Presence)\n",
        "    current_suitability = predictors.select(CONFIG['BANDS']).classify(rf_model).arrayGet([1])\n",
        "    Map.addLayer(current_suitability.clip(aoi), CONFIG['VISUALIZATION']['SUITABILITY'], \"Current Suitability (2000-2020)\")\n",
        "\n",
        "if 'future_maps' in locals():\n",
        "    for name, img in future_maps.items():\n",
        "        Map.addLayer(img.clip(aoi), CONFIG['VISUALIZATION']['SUITABILITY'], f\"Suitability {name}\")\n",
        "\n",
        "    if 'diff_map' in locals():\n",
        "        Map.addLayer(diff_map.clip(aoi), CONFIG['VISUALIZATION']['DIFF'], \"Diff SSP585-SSP245 (2050)\")\n",
        "\n",
        "Map\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 6. Exports\n",
        "print(\"Starting export tasks...\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Export Current Suitability\n",
        "if 'rf_model' in locals() and 'predictors' in locals():\n",
        "    print(\"Exporting Current Suitability...\")\n",
        "    # Ensure current_suitability exists (re-calc if needed)\n",
        "    current_suitability = predictors.select(CONFIG['BANDS']).classify(rf_model).arrayGet([1])\n",
        "    export_image_to_drive(current_suitability, 'export_current_suitability', 'avocado_current', aoi)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. Export Future Maps\n",
        "if 'future_maps' in locals():\n",
        "    for name, img in future_maps.items():\n",
        "        print(f\"Exporting {name}...\")\n",
        "        export_image_to_drive(img, f'export_{name}', f'avocado_{name}', aoi)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 7. Check Task Status\n",
        "print(\"Checking recent Earth Engine tasks...\")\n",
        "tasks = ee.batch.Task.list()\n",
        "for task in tasks[:10]:  # List last 10 tasks\n",
        "    print(f\"Task: {task.config['description']}, Status: {task.state}, ID: {task.id}\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}