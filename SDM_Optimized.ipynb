{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Optimized Species Distribution Modeling (SDM)\n",
        "\n",
        "This notebook contains a streamlined and modularized version of the SDM workflow. \n",
        "It includes steps for initialization, data preparation, model training, prediction, and export."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Setup and Imports\n",
        "import ee\n",
        "import geemap\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.cloud import bigquery\n",
        "import geopandas as gpd\n",
        "import shapely.geometry\n",
        "import json\n",
        "\n",
        "# Initialize Earth Engine\n",
        "try:\n",
        "    ee.Initialize(project='cryptic-yen-457008-p4')\n",
        "except Exception as e:\n",
        "    ee.Authenticate()\n",
        "    ee.Initialize(project='cryptic-yen-457008-p4')\n",
        "\n",
        "#  Initialize BigQuery\n",
        "client = bigquery.Client(project='rsc-cropmap-lzp')\n",
        "\n",
        "# 2. Run the Query\n",
        "sql = \"\"\"\n",
        "SELECT\n",
        "    pred1 AS species,\n",
        "    CAST(agri_year AS INT64) AS year,\n",
        "    ST_AsGeoJSON(geometry) AS geometry\n",
        "FROM\n",
        "    `rsc-cropmap-lzp.published.Fused_Categories_Orchards`\n",
        "WHERE\n",
        "    pred1 IS NOT NULL\n",
        "    AND pred1 = 'avocado'\n",
        "\"\"\"\n",
        "print(\"Running BigQuery...\")\n",
        "df = client.query(sql).to_dataframe(create_bqstorage_client=False)\n",
        "\n",
        "# 3. Convert to GeoDataFrame\n",
        "df['geometry'] = df['geometry'].apply(lambda x: shapely.geometry.shape(json.loads(x)))\n",
        "gdf = gpd.GeoDataFrame(df, geometry='geometry', crs=\"EPSG:4326\")\n",
        "\n",
        "# 4. Convert to Earth Engine FeatureCollection (Critical step for SDM)\n",
        "print(\"Converting to Earth Engine object...\")\n",
        "data_raw = geemap.gdf_to_ee(gdf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. Configuration\n",
        "# Centralize all constants and paths here for easy management\n",
        "\n",
        "CONFIG = {\n",
        "    'BANDS': ['OrderST', 'aspect', 'elevation', 'slope', 'bio01', 'bio12', 'srad', 'vdf'],\n",
        "    'ASSETS': {\n",
        "        'SOIL': \"projects/cryptic-yen-457008-p4/assets/IsraelSoilTaxonomy\",\n",
        "        'BIO1': \"projects/cryptic-yen-457008-p4/assets/SDM/wc2_1_30s_bio_1\",\n",
        "        'BIO12': \"projects/cryptic-yen-457008-p4/assets/SDM/wc2_1_30s_bio_12\",\n",
        "        'SRAD': \"projects/cryptic-yen-457008-p4/assets/SDM/wc2_1_30s_srad\",\n",
        "        'VAPR': \"projects/cryptic-yen-457008-p4/assets/SDM/wc2_1_30s_vapr\",\n",
        "        'MODEL_OUTPUT': 'projects/cryptic-yen-457008-p4/assets/avocado_final_model'\n",
        "    },\n",
        "    'CMIP6_COLLECTION': \"NASA/GDDP-CMIP6\",\n",
        "    'VISUALIZATION': {\n",
        "        'SUITABILITY': {\"min\": 0, \"max\": 1, \"palette\": [\"ffffff\", \"cecece\", \"fcd163\", \"66a000\", \"204200\"]},\n",
        "        'DIFF': {\"min\": -0.3, \"max\": 0.3, \"palette\": [\"d7191c\", \"ffffff\", \"2c7bb6\"]}\n",
        "    },\n",
        "    'EXPORT': {\n",
        "        'FOLDER': 'GEE_Exports',\n",
        "        'SCALE': 1000\n",
        "    },\n",
        "    'GRAIN_SIZE': 1000,\n",
        "    'TEST_YEAR': 2018\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. Helper Functions\n",
        "\n",
        "def get_predictors():\n",
        "    \"\"\"Loads and preprocesses predictor variables.\"\"\"\n",
        "    # Topography\n",
        "    terrain = ee.Algorithms.Terrain(ee.Image(\"USGS/SRTMGL1_003\")).unmask()\n",
        "    \n",
        "    # Soil\n",
        "    soil_fc = ee.FeatureCollection(CONFIG['ASSETS']['SOIL'])\n",
        "    u_types = soil_fc.aggregate_array('OrderST').distinct().sort()\n",
        "    soil_img = soil_fc.map(lambda f: f.set('Code', u_types.indexOf(f.get('OrderST')))) \\\n",
        "        .reduceToImage(['Code'], ee.Reducer.first()).rename('OrderST').unmask(-1)\n",
        "    \n",
        "    # Climate (Current)\n",
        "    # Load new assets\n",
        "    bio1 = ee.Image(CONFIG['ASSETS']['BIO1']).rename('bio01').unmask()\n",
        "    bio12 = ee.Image(CONFIG['ASSETS']['BIO12']).rename('bio12').unmask()\n",
        "    srad = ee.Image(CONFIG['ASSETS']['SRAD']).rename('srad').unmask()\n",
        "    vapr = ee.Image(CONFIG['ASSETS']['VAPR']).rename('vapr').unmask()\n",
        "\n",
        "    # Calculate VDF (Vapor Pressure Deficit)\n",
        "    # VPD = es - ea (vapr)\n",
        "    # es = 0.6108 * exp(17.27 * T / (T + 237.3))\n",
        "    # bio1 is Mean Temp (check units, assuming Celsius based on standard WorldClim)\n",
        "    \n",
        "    es = bio1.expression(\n",
        "        '0.6108 * exp((17.27 * T) / (T + 237.3))',\n",
        "        {'T': bio1}\n",
        "    )\n",
        "    \n",
        "    # vapr is in kPa, es is in kPa (0.6108 is kPa)\n",
        "    vdf = es.subtract(vapr).rename('vdf')\n",
        "\n",
        "    # Combine all\n",
        "    return bio1.addBands([bio12, srad, vdf, soil_img, terrain.select(['elevation', 'slope', 'aspect'])])\n",
        "\n",
        "def remove_duplicates(data, grain_size):\n",
        "    \"\"\"Removes duplicate presence points within the same pixel.\"\"\"\n",
        "    random_raster = ee.Image.random().reproject(\"EPSG:4326\", None, grain_size)\n",
        "    rand_point_vals = random_raster.sampleRegions(\n",
        "        collection=ee.FeatureCollection(data), geometries=True\n",
        "    )\n",
        "    return rand_point_vals.distinct(\"random\")\n",
        "\n",
        "def split_data(data, predictors, aoi, test_year, grain_size):\n",
        "    \"\"\"Splits data into Train, Validation, and Test sets with pseudo-absences.\"\"\"\n",
        "    \n",
        "    # 1. De-duplicate Presence\n",
        "    print(\"Removing duplicates...\")\n",
        "    presence = remove_duplicates(data, grain_size)\n",
        "    print(f\"Presence points after de-duplication: {presence.size().getInfo()}\")\n",
        "\n",
        "    # 2. Split Presence by Year and Random\n",
        "    # Test Set (Hold out year)\n",
        "    pres_test = presence.filter(ee.Filter.eq('year', test_year)).map(lambda f: f.set('PresAbs', 1))\n",
        "    \n",
        "    # Remaining (Train + Val)\n",
        "    pres_remain = presence.filter(ee.Filter.neq('year', test_year))\n",
        "    pres_remain = pres_remain.randomColumn()\n",
        "    \n",
        "    # Train (70%) / Val (30%)\n",
        "    pres_train = pres_remain.filter(ee.Filter.lt('random', 0.7)).map(lambda f: f.set('PresAbs', 1))\n",
        "    pres_val = pres_remain.filter(ee.Filter.gte('random', 0.7)).map(lambda f: f.set('PresAbs', 1))\n",
        "    \n",
        "    # 3. Generate Pseudo-Absences\n",
        "    print(\"Generating pseudo-absences...\")\n",
        "    \n",
        "    # Presence mask (user logic)\n",
        "    presence_mask = presence.reduceToImage(properties=['random'], reducer=ee.Reducer.first()) \\\n",
        "        .reproject('EPSG:4326', None, grain_size).mask().neq(1).selfMask()\n",
        "        \n",
        "    # Valid predictor area (mask of first band)\n",
        "    cl_mask = predictors.select(0).mask()\n",
        "    \n",
        "    # Area for Pseudo-Absences\n",
        "    area_for_pa = presence_mask.updateMask(cl_mask).clip(aoi)\n",
        "    \n",
        "    # Generate absences (Total count approx equal to total presence)\n",
        "    total_pres_count = presence.size()\n",
        "    absences = predictors.sample(\n",
        "        region=area_for_pa.geometry(), \n",
        "        scale=grain_size, \n",
        "        numPixels=total_pres_count.multiply(1.2), # Generate a bit more to be safe\n",
        "        geometries=True\n",
        "    ).randomColumn().map(lambda f: f.set('PresAbs', 0))\n",
        "    \n",
        "    # Split Absences to match Presence ratios\n",
        "    # We want roughly 1:1 ratio in each set\n",
        "    count_test = pres_test.size()\n",
        "    count_train = pres_train.size()\n",
        "    \n",
        "    # Sort by random to easily pick chunks\n",
        "    absences_list = absences.toList(absences.size())\n",
        "    \n",
        "    abs_test = ee.FeatureCollection(absences_list.slice(0, count_test))\n",
        "    abs_train = ee.FeatureCollection(absences_list.slice(count_test, count_test.add(count_train)))\n",
        "    abs_val = ee.FeatureCollection(absences_list.slice(count_test.add(count_train)))\n",
        "    \n",
        "    # 4. Merge and Sample\n",
        "    def sample_data(pres, abs_):\n",
        "        merged = pres.merge(abs_)\n",
        "        return predictors.select(CONFIG['BANDS']).sampleRegions(\n",
        "            collection=merged, \n",
        "            properties=[\"PresAbs\"], \n",
        "            scale=grain_size, \n",
        "            tileScale=16\n",
        "        )\n",
        "\n",
        "    train_data = sample_data(pres_train, abs_train)\n",
        "    val_data = sample_data(pres_val, abs_val)\n",
        "    test_data = sample_data(pres_test, abs_test)\n",
        "    \n",
        "    return train_data, val_data, test_data\n",
        "\n",
        "def train_model(training_data, mode='MULTIPROBABILITY'):\n",
        "    \"\"\"Trains the Random Forest classifier.\"\"\"\n",
        "    classifier = ee.Classifier.smileRandomForest(250).train(training_data, \"PresAbs\", CONFIG['BANDS'])\n",
        "    return classifier.setOutputMode(mode)\n",
        "\n",
        "def get_future_climate(scenario, model='ACCESS1-0', year=2050):\n",
        "    \"\"\"Fetches and processes future climate data.\"\"\"\n",
        "    # ... (To be updated in next step, but keeping placeholder for now to avoid breaking)\n",
        "    # For now, just return what was there or updated logic if I want to do it all at once.\n",
        "    # Let's do it all at once to be efficient.\n",
        "    \n",
        "    start_year = year - 9\n",
        "    end_year = year + 10\n",
        "    \n",
        "    nex = ee.ImageCollection(CONFIG['CMIP6_COLLECTION']) \\\n",
        "        .filter(ee.Filter.date(f'{start_year}-01-01', f'{end_year}-12-31')) \\\n",
        "        .filter(ee.Filter.eq('scenario', scenario)) \\\n",
        "        .filter(ee.Filter.eq('model', model))\n",
        "\n",
        "    def convert(img):\n",
        "        # Pr is in kg m-2 s-1 (mm/s). Multiply by 86400 to get mm/day.\n",
        "        pr = img.select('pr').multiply(86400).rename('precip_mm')\n",
        "        \n",
        "        # Tas is in Kelvin. Subtract 273.15 to get Celsius.\n",
        "        tas = img.select('tas').subtract(273.15).rename('tmean_c')\n",
        "        \n",
        "        # Rsds is in W m-2. Convert to kJ m-2 day-1.\n",
        "        # 1 W = 1 J/s. 1 day = 86400 s.\n",
        "        # W/m2 * 86400 = J/m2/day. Divide by 1000 for kJ.\n",
        "        # Factor = 86.4\n",
        "        srad = img.select('rsds').multiply(86.4).rename('srad')\n",
        "        \n",
        "        # Calculate VPD using Tas and Hurs (%)\n",
        "        # es = 0.6108 * exp(17.27 * T / (T + 237.3))\n",
        "        # ea = es * (hurs / 100)\n",
        "        # vpd = es - ea\n",
        "        \n",
        "        t = tas\n",
        "        hurs = img.select('hurs')\n",
        "        \n",
        "        es = t.expression(\n",
        "            '0.6108 * exp((17.27 * T) / (T + 237.3))',\n",
        "            {'T': t}\n",
        "        )\n",
        "        \n",
        "        ea = es.multiply(hurs.divide(100))\n",
        "        vdf = es.subtract(ea).rename('vdf')\n",
        "        \n",
        "        return img.addBands([pr, tas, srad, vdf])\n",
        "\n",
        "    nex_agg = nex.map(convert).map(lambda i: i.resample('bilinear').reproject('EPSG:4326', None, 1000))\n",
        "    \n",
        "    # Aggregate over time (mean for most, sum for precip if needed, but usually mean annual precip? \n",
        "    # Bio12 is Annual Precip. So we need sum of daily precip for a year, then mean over 20 years?\n",
        "    # Or mean daily precip * 365?\n",
        "    # WorldClim Bio12 is Annual Precipitation.\n",
        "    # CMIP6 'pr' converted to mm/day.\n",
        "    # Mean(mm/day) * 365 = Annual Precip.\n",
        "    \n",
        "    bio01 = nex_agg.select('tmean_c').mean().rename('bio01')\n",
        "    bio12 = nex_agg.select('precip_mm').mean().multiply(365).rename('bio12')\n",
        "    srad = nex_agg.select('srad').mean().rename('srad')\n",
        "    vdf = nex_agg.select('vdf').mean().rename('vdf')\n",
        "    \n",
        "    return bio01.addBands([bio12, srad, vdf])\n",
        "\n",
        "def export_image_to_drive(image, description, filename, region):\n",
        "    \"\"\"Creates and starts an export task.\"\"\"\n",
        "    task = ee.batch.Export.image.toDrive(\n",
        "        image=image,\n",
        "        description=description,\n",
        "        folder=CONFIG['EXPORT']['FOLDER'],\n",
        "        fileNamePrefix=filename,\n",
        "        scale=CONFIG['EXPORT']['SCALE'],\n",
        "        region=region,\n",
        "        maxPixels=1e13\n",
        "    )\n",
        "    task.start()\n",
        "    print(f\"Started export task: {description}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4. Main Execution Flow\n",
        "\n",
        "# --- A. Prepare Data ---\n",
        "print(\"Preparing data...\")\n",
        "predictors = get_predictors()\n",
        "soil_fc = ee.FeatureCollection(CONFIG['ASSETS']['SOIL'])\n",
        "aoi = soil_fc.geometry().bounds()\n",
        "\n",
        "if 'data_raw' in locals():\n",
        "    print(\"Splitting data and preparing datasets...\")\n",
        "    # data_raw is the raw EE FeatureCollection from BigQuery\n",
        "    train_data, val_data, test_data = split_data(\n",
        "        data_raw, predictors, aoi, CONFIG['TEST_YEAR'], CONFIG['GRAIN_SIZE']\n",
        "    )\n",
        "    \n",
        "    print(f\"Training set size: {train_data.size().getInfo()}\")\n",
        "    print(f\"Validation set size: {val_data.size().getInfo()}\")\n",
        "    print(f\"Test set size: {test_data.size().getInfo()}\")\n",
        "else:\n",
        "    print(\"WARNING: 'data_raw' variable not found. Please ensure BigQuery step ran successfully.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Data Analysis ---\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "if 'train_data' in locals():\n",
        "    print(\"Generating Correlation Matrix...\")\n",
        "    # Sample training data to pandas for analysis (limit to 5000 points to avoid timeouts)\n",
        "    # We need to sample the predictors at the training points\n",
        "    # Actually, train_data already has the predictor values if sampled correctly?\n",
        "    # In split_data, we used sampleRegions, so the properties should be there.\n",
        "    \n",
        "    # Convert to Pandas\n",
        "    # Limit size\n",
        "    n_samples = min(5000, train_data.size().getInfo())\n",
        "    df_train = geemap.ee_to_pandas(train_data.limit(n_samples))\n",
        "    \n",
        "    # Select predictor columns\n",
        "    cols = CONFIG['BANDS']\n",
        "    \n",
        "    # Plot\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(df_train[cols].corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "    plt.title(\"Predictor Correlation Matrix\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- B. Train Model ---\n",
        "if 'train_data' in locals():\n",
        "    print(\"Training model...\")\n",
        "    rf_model = train_model(train_data)\n",
        "    \n",
        "    # --- Validation ---\n",
        "    print(\"Validating model...\")\n",
        "    validated = val_data.classify(rf_model)\n",
        "    # Calculate Accuracy (or other metrics)\n",
        "    error_matrix = validated.errorMatrix('PresAbs', 'classification')\n",
        "    print(\"Validation Accuracy:\", error_matrix.accuracy().getInfo())\n",
        "    print(\"Validation Kappa:\", error_matrix.kappa().getInfo())\n",
        "    \n",
        "    # --- Testing ---\n",
        "    print(f\"Testing on year {CONFIG['TEST_YEAR']}...\")\n",
        "    tested = test_data.classify(rf_model)\n",
        "    test_matrix = tested.errorMatrix('PresAbs', 'classification')\n",
        "    print(\"Test Accuracy:\", test_matrix.accuracy().getInfo())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Model Interpretation ---\n",
        "if 'rf_model' in locals():\n",
        "    print(\"Calculating Variable Importance...\")\n",
        "    importance = rf_model.explain().get('importance').getInfo()\n",
        "    \n",
        "    # Plot\n",
        "    import matplotlib.pyplot as plt\n",
        "    \n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.bar(importance.keys(), importance.values())\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.title(\"Variable Importance\")\n",
        "    plt.ylabel(\"Importance\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- C. Future Predictions ---\n",
        "\n",
        "def predict_suitability(model, climate_stack, static_predictors):\n",
        "    full_stack = climate_stack.addBands(static_predictors.select(['OrderST', 'elevation', 'slope', 'aspect']))\n",
        "    return full_stack.select(CONFIG['BANDS']).classify(model).arrayGet([1])\n",
        "\n",
        "if 'rf_model' in locals():\n",
        "    print(\"Predicting future scenarios...\")\n",
        "    \n",
        "    scenarios = ['ssp245', 'ssp585']\n",
        "    years = [2050, 2100]\n",
        "    \n",
        "    future_maps = {}\n",
        "    \n",
        "    for scenario in scenarios:\n",
        "        for year in years:\n",
        "            print(f\"Processing {scenario} - {year}...\")\n",
        "            future_climate = get_future_climate(scenario, year=year)\n",
        "            suitability_map = predict_suitability(rf_model, future_climate, predictors)\n",
        "            future_maps[f\"{scenario}_{year}\"] = suitability_map\n",
        "            \n",
        "    # Calculate Difference (Example: SSP585 2050 vs SSP245 2050)\n",
        "    if 'ssp585_2050' in future_maps and 'ssp245_2050' in future_maps:\n",
        "        diff_map = future_maps['ssp585_2050'].subtract(future_maps['ssp245_2050'])\n",
        "        print(\"Difference map created.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5. Visualization\n",
        "Map = geemap.Map(layout={'height':'600px', 'width':'100%'})\n",
        "Map.centerObject(aoi, 7)\n",
        "\n",
        "if 'future_maps' in locals():\n",
        "    for name, img in future_maps.items():\n",
        "        Map.addLayer(img.clip(aoi), CONFIG['VISUALIZATION']['SUITABILITY'], f\"Suitability {name}\")\n",
        "        \n",
        "    if 'diff_map' in locals():\n",
        "        Map.addLayer(diff_map.clip(aoi), CONFIG['VISUALIZATION']['DIFF'], \"Diff SSP585-SSP245 (2050)\")\n",
        "\n",
        "Map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 6. Exports\n",
        "if 'future_maps' in locals():\n",
        "    for name, img in future_maps.items():\n",
        "        # export_image_to_drive(img, f'export_{name}', f'avocado_{name}', aoi)\n",
        "        pass\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}