import json

notebook_content = {
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimized Species Distribution Modeling (SDM)\n",
    "\n",
    "This notebook contains a streamlined and modularized version of the SDM workflow. \n",
    "It includes steps for initialization, data preparation, model training, prediction, and export."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": None,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Setup and Imports\n",
    "import ee\n",
    "import geemap\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from google.cloud import bigquery\n",
    "import geopandas as gpd\n",
    "import shapely.geometry\n",
    "import json\n",
    "\n",
    "# Initialize Earth Engine\n",
    "try:\n",
    "    ee.Initialize(project='cryptic-yen-457008-p4')\n",
    "except Exception as e:\n",
    "    ee.Authenticate()\n",
    "    ee.Initialize(project='cryptic-yen-457008-p4')\n",
    "\n",
    "#  Initialize BigQuery\n",
    "client = bigquery.Client(project='rsc-cropmap-lzp')\n",
    "\n",
    "# 2. Run the Query\n",
    "sql = \"\"\"\n",
    "SELECT\n",
    "    pred1 AS species,\n",
    "    CAST(agri_year AS INT64) AS year,\n",
    "    ST_AsGeoJSON(geometry) AS geometry\n",
    "FROM\n",
    "    `rsc-cropmap-lzp.published.Fused_Categories_Orchards`\n",
    "WHERE\n",
    "    pred1 IS NOT NULL\n",
    "    AND pred1 = 'avocado'\n",
    "\"\"\"\n",
    "print(\"Running BigQuery...\")\n",
    "df = client.query(sql).to_dataframe(create_bqstorage_client=False)\n",
    "\n",
    "# 3. Convert to GeoDataFrame\n",
    "df['geometry'] = df['geometry'].apply(lambda x: shapely.geometry.shape(json.loads(x)))\n",
    "gdf = gpd.GeoDataFrame(df, geometry='geometry', crs=\"EPSG:4326\")\n",
    "\n",
    "# 4. Convert to Earth Engine FeatureCollection (Critical step for SDM)\n",
    "print(\"Converting to Earth Engine object...\")\n",
    "data_raw = geemap.gdf_to_ee(gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": None,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Configuration\n",
    "# Centralize all constants and paths here for easy management\n",
    "\n",
    "CONFIG = {\n",
    "    'BANDS': ['OrderST', 'aspect', 'elevation', 'slope', 'bio01', 'bio12'],\n",
    "    'ASSETS': {\n",
    "        'SOIL': \"projects/cryptic-yen-457008-p4/assets/IsraelSoilTaxonomy\",\n",
    "        'MODEL_OUTPUT': 'projects/cryptic-yen-457008-p4/assets/avocado_final_model'\n",
    "    },\n",
    "    'VISUALIZATION': {\n",
    "        'SUITABILITY': {\"min\": 0, \"max\": 1, \"palette\": [\"ffffff\", \"cecece\", \"fcd163\", \"66a000\", \"204200\"]},\n",
    "        'DIFF': {\"min\": -0.3, \"max\": 0.3, \"palette\": [\"d7191c\", \"ffffff\", \"2c7bb6\"]}\n",
    "    },\n",
    "    'EXPORT': {\n",
    "        'FOLDER': 'GEE_Exports',\n",
    "        'SCALE': 1000\n",
    "    },\n",
    "    'GRAIN_SIZE': 1000,\n",
    "    'TEST_YEAR': 2018\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": None,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Helper Functions\n",
    "\n",
    "def get_predictors():\n",
    "    \"\"\"Loads and preprocesses predictor variables.\"\"\"\n",
    "    # Topography\n",
    "    terrain = ee.Algorithms.Terrain(ee.Image(\"USGS/SRTMGL1_003\")).unmask()\n",
    "    \n",
    "    # Soil\n",
    "    soil_fc = ee.FeatureCollection(CONFIG['ASSETS']['SOIL'])\n",
    "    u_types = soil_fc.aggregate_array('OrderST').distinct().sort()\n",
    "    soil_img = soil_fc.map(lambda f: f.set('Code', u_types.indexOf(f.get('OrderST')))) \\\n",
    "        .reduceToImage(['Code'], ee.Reducer.first()).rename('OrderST').unmask(-1)\n",
    "    \n",
    "    # Climate (Current)\n",
    "    bio_curr = ee.Image(\"WORLDCLIM/V1/BIO\").select(['bio01', 'bio12']).unmask()\n",
    "    \n",
    "    # Combine all\n",
    "    return bio_curr.addBands([soil_img, terrain.select(['elevation', 'slope', 'aspect'])])\n",
    "\n",
    "def remove_duplicates(data, grain_size):\n",
    "    \"\"\"Removes duplicate presence points within the same pixel.\"\"\"\n",
    "    random_raster = ee.Image.random().reproject(\"EPSG:4326\", None, grain_size)\n",
    "    rand_point_vals = random_raster.sampleRegions(\n",
    "        collection=ee.FeatureCollection(data), geometries=True\n",
    "    )\n",
    "    return rand_point_vals.distinct(\"random\")\n",
    "\n",
    "def split_data(data, predictors, aoi, test_year, grain_size):\n",
    "    \"\"\"Splits data into Train, Validation, and Test sets with pseudo-absences.\"\"\"\n",
    "    \n",
    "    # 1. De-duplicate Presence\n",
    "    print(\"Removing duplicates...\")\n",
    "    presence = remove_duplicates(data, grain_size)\n",
    "    print(f\"Presence points after de-duplication: {presence.size().getInfo()}\")\n",
    "\n",
    "    # 2. Split Presence by Year and Random\n",
    "    # Test Set (Hold out year)\n",
    "    pres_test = presence.filter(ee.Filter.eq('year', test_year)).map(lambda f: f.set('PresAbs', 1))\n",
    "    \n",
    "    # Remaining (Train + Val)\n",
    "    pres_remain = presence.filter(ee.Filter.neq('year', test_year))\n",
    "    pres_remain = pres_remain.randomColumn()\n",
    "    \n",
    "    # Train (70%) / Val (30%)\n",
    "    pres_train = pres_remain.filter(ee.Filter.lt('random', 0.7)).map(lambda f: f.set('PresAbs', 1))\n",
    "    pres_val = pres_remain.filter(ee.Filter.gte('random', 0.7)).map(lambda f: f.set('PresAbs', 1))\n",
    "    \n",
    "    # 3. Generate Pseudo-Absences\n",
    "    print(\"Generating pseudo-absences...\")\n",
    "    \n",
    "    # Presence mask (user logic)\n",
    "    presence_mask = presence.reduceToImage(properties=['random'], reducer=ee.Reducer.first()) \\\n",
    "        .reproject('EPSG:4326', None, grain_size).mask().neq(1).selfMask()\n",
    "        \n",
    "    # Valid predictor area (mask of first band)\n",
    "    cl_mask = predictors.select(0).mask()\n",
    "    \n",
    "    # Area for Pseudo-Absences\n",
    "    area_for_pa = presence_mask.updateMask(cl_mask).clip(aoi)\n",
    "    \n",
    "    # Generate absences (Total count approx equal to total presence)\n",
    "    total_pres_count = presence.size()\n",
    "    absences = predictors.sample(\n",
    "        region=area_for_pa.geometry(), \n",
    "        scale=grain_size, \n",
    "        numPixels=total_pres_count.multiply(1.2), # Generate a bit more to be safe\n",
    "        geometries=True\n",
    "    ).randomColumn().map(lambda f: f.set('PresAbs', 0))\n",
    "    \n",
    "    # Split Absences to match Presence ratios\n",
    "    # We want roughly 1:1 ratio in each set\n",
    "    count_test = pres_test.size()\n",
    "    count_train = pres_train.size()\n",
    "    \n",
    "    # Sort by random to easily pick chunks\n",
    "    absences_list = absences.toList(absences.size())\n",
    "    \n",
    "    abs_test = ee.FeatureCollection(absences_list.slice(0, count_test))\n",
    "    abs_train = ee.FeatureCollection(absences_list.slice(count_test, count_test.add(count_train)))\n",
    "    abs_val = ee.FeatureCollection(absences_list.slice(count_test.add(count_train)))\n",
    "    \n",
    "    # 4. Merge and Sample\n",
    "    def sample_data(pres, abs_):\n",
    "        merged = pres.merge(abs_)\n",
    "        return predictors.select(CONFIG['BANDS']).sampleRegions(\n",
    "            collection=merged, \n",
    "            properties=[\"PresAbs\"], \n",
    "            scale=grain_size, \n",
    "            tileScale=16\n",
    "        )\n",
    "\n",
    "    train_data = sample_data(pres_train, abs_train)\n",
    "    val_data = sample_data(pres_val, abs_val)\n",
    "    test_data = sample_data(pres_test, abs_test)\n",
    "    \n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "def train_model(training_data, mode='MULTIPROBABILITY'):\n",
    "    \"\"\"Trains the Random Forest classifier.\"\"\"\n",
    "    classifier = ee.Classifier.smileRandomForest(250).train(training_data, \"PresAbs\", CONFIG['BANDS'])\n",
    "    return classifier.setOutputMode(mode)\n",
    "\n",
    "def get_future_climate(scenario, model='ACCESS1-0', year=2050):\n",
    "    \"\"\"Fetches and processes future climate data.\"\"\"\n",
    "    nex = ee.ImageCollection(\"NASA/NEX-GDDP\") \\\n",
    "        .filter(ee.Filter.date(f'{year}-01-01', f'{year}-12-31')) \\\n",
    "        .filter(ee.Filter.eq('scenario', scenario)) \\\n",
    "        .filter(ee.Filter.eq('model', model))\n",
    "\n",
    "    def convert(img):\n",
    "        pr = img.select('pr').multiply(86400).rename('precip_mm')\n",
    "        tm = img.select('tasmin').add(img.select('tasmax')).divide(2).subtract(273.15).rename('tmean_c')\n",
    "        return img.addBands([pr, tm])\n",
    "\n",
    "    nex_agg = nex.map(convert).map(lambda i: i.resample('bilinear').reproject('EPSG:4326', None, 1000))\n",
    "    bio01 = nex_agg.select('tmean_c').mean().rename('bio01')\n",
    "    bio12 = nex_agg.select('precip_mm').sum().rename('bio12')\n",
    "    \n",
    "    return bio01.addBands(bio12)\n",
    "\n",
    "def export_image_to_drive(image, description, filename, region):\n",
    "    \"\"\"Creates and starts an export task.\"\"\"\n",
    "    task = ee.batch.Export.image.toDrive(\n",
    "        image=image,\n",
    "        description=description,\n",
    "        folder=CONFIG['EXPORT']['FOLDER'],\n",
    "        fileNamePrefix=filename,\n",
    "        scale=CONFIG['EXPORT']['SCALE'],\n",
    "        region=region,\n",
    "        maxPixels=1e13\n",
    "    )\n",
    "    task.start()\n",
    "    print(f\"Started export task: {description}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": None,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Main Execution Flow\n",
    "\n",
    "# --- A. Prepare Data ---\n",
    "print(\"Preparing data...\")\n",
    "predictors = get_predictors()\n",
    "soil_fc = ee.FeatureCollection(CONFIG['ASSETS']['SOIL'])\n",
    "aoi = soil_fc.geometry().bounds()\n",
    "\n",
    "# --- B. Train Model ---\n",
    "if 'data_raw' in locals():\n",
    "    print(\"Splitting data and preparing datasets...\")\n",
    "    # data_raw is the raw EE FeatureCollection from BigQuery\n",
    "    train_data, val_data, test_data = split_data(\n",
    "        data_raw, predictors, aoi, CONFIG['TEST_YEAR'], CONFIG['GRAIN_SIZE']\n",
    "    )\n",
    "    \n",
    "    print(f\"Training set size: {train_data.size().getInfo()}\")\n",
    "    print(f\"Validation set size: {val_data.size().getInfo()}\")\n",
    "    print(f\"Test set size: {test_data.size().getInfo()}\")\n",
    "\n",
    "    print(\"Training model...\")\n",
    "    rf_model = train_model(train_data)\n",
    "    \n",
    "    # --- Validation ---\n",
    "    print(\"Validating model...\")\n",
    "    validated = val_data.classify(rf_model)\n",
    "    # Calculate Accuracy (or other metrics)\n",
    "    error_matrix = validated.errorMatrix('PresAbs', 'classification')\n",
    "    print(\"Validation Accuracy:\", error_matrix.accuracy().getInfo())\n",
    "    print(\"Validation Kappa:\", error_matrix.kappa().getInfo())\n",
    "    \n",
    "    # --- Testing ---\n",
    "    print(f\"Testing on year {CONFIG['TEST_YEAR']}...\")\n",
    "    tested = test_data.classify(rf_model)\n",
    "    test_matrix = tested.errorMatrix('PresAbs', 'classification')\n",
    "    print(\"Test Accuracy:\", test_matrix.accuracy().getInfo())\n",
    "    \n",
    "else:\n",
    "    print(\"WARNING: 'data_raw' variable not found. Skipping training.\")\n",
    "\n",
    "# --- C. Future Predictions ---\n",
    "# Assuming we have a trained model (rf_model). \n",
    "# If you are loading a saved model:\n",
    "# rf_model = ee.Classifier.load(CONFIG['ASSETS']['MODEL_OUTPUT'])\n",
    "\n",
    "# For demonstration, let's define the prediction logic wrapper\n",
    "def predict_suitability(model, climate_stack, static_predictors):\n",
    "    full_stack = climate_stack.addBands(static_predictors.select(['OrderST', 'elevation', 'slope', 'aspect']))\n",
    "    return full_stack.select(CONFIG['BANDS']).classify(model).arrayGet([1])\n",
    "\n",
    "# Example usage (commented out until model is ready):\n",
    "# print(\"Predicting future scenarios...\")\n",
    "# future_climate_rcp45 = get_future_climate('rcp45')\n",
    "# map_rcp45 = predict_suitability(rf_model, future_climate_rcp45, predictors)\n",
    "\n",
    "# future_climate_rcp85 = get_future_climate('rcp85')\n",
    "# map_rcp85 = predict_suitability(rf_model, future_climate_rcp85, predictors)\n",
    "\n",
    "# diff_map = map_rcp85.subtract(map_rcp45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": None,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Visualization\n",
    "Map = geemap.Map(layout={'height':'600px', 'width':'100%'})\n",
    "Map.centerObject(aoi, 7)\n",
    "\n",
    "# Add layers (Uncomment when maps are generated)\n",
    "# Map.addLayer(map_rcp45.clip(aoi), CONFIG['VISUALIZATION']['SUITABILITY'], \"Future Suitability 2050 (RCP 4.5)\")\n",
    "# Map.addLayer(map_rcp85.clip(aoi), CONFIG['VISUALIZATION']['SUITABILITY'], \"Future Suitability 2050 (RCP 8.5)\")\n",
    "# Map.addLayer(diff_map.clip(aoi), CONFIG['VISUALIZATION']['DIFF'], \"Difference (Red=Worse in 8.5)\")\n",
    "\n",
    "# Map.add_colorbar(CONFIG['VISUALIZATION']['SUITABILITY'], label=\"Suitability Probability\", layer_name=\"Future Suitability\")\n",
    "# Map.add_colorbar(CONFIG['VISUALIZATION']['DIFF'], label=\"Change (Negative=Loss)\", layer_name=\"Difference\")\n",
    "\n",
    "Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": None,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Exports\n",
    "# export_image_to_drive(map_rcp45, 'export_rcp45', 'avocado_rcp45_2050', aoi)\n",
    "# export_image_to_drive(map_rcp85, 'export_rcp85', 'avocado_rcp85_2050', aoi)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

with open('SDM_Optimized.ipynb', 'w', encoding='utf-8') as f:
    json.dump(notebook_content, f, indent=2)

print("SDM_Optimized.ipynb created successfully.")
